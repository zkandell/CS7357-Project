{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This currently does not function, in the middle of a rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "# Get pandas for data manipulation\n",
    "import pandas as pd\n",
    "# Import nltk for text processing\n",
    "import nltk\n",
    "# Import os for file manipulation\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Title  \\\n",
      "0                             # (2012/II)   \n",
      "1          #1 Cheerleader Camp (2010) (V)   \n",
      "2                 #1 Serial Killer (2013)   \n",
      "3  #1 at the Apocalypse Box Office (2015)   \n",
      "4                             #137 (2011)   \n",
      "\n",
      "                                                Plot  Action  Adventure  \\\n",
      "0  The night falls on the big city and a hooded f...       0          0   \n",
      "1  When they're hired to work at a cheerleading c...       0          0   \n",
      "2  Years of seething rage against the racism he's...       0          0   \n",
      "3  Jules is, self declared, the most useless pers...       0          0   \n",
      "4  #137 is a SCI/FI thriller about a girl, Marla,...       0          0   \n",
      "\n",
      "   Animation  Biography  Comedy  Crime  Drama  Family  ...  Horror  Music  \\\n",
      "0          1          0       0      0      0       0  ...       0      0   \n",
      "1          0          0       1      0      0       0  ...       0      0   \n",
      "2          0          0       0      0      0       0  ...       1      0   \n",
      "3          0          0       1      0      0       0  ...       0      0   \n",
      "4          0          0       0      0      0       0  ...       0      0   \n",
      "\n",
      "   Musical  Mystery  Romance  Sci-Fi  Sport  Thriller  War  Western  \n",
      "0        0        0        0       0      0         0    0        0  \n",
      "1        0        0        0       0      0         0    0        0  \n",
      "2        0        0        0       0      0         0    0        0  \n",
      "3        0        0        0       1      0         0    0        0  \n",
      "4        0        0        0       1      0         0    0        0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the data from Datasets/onehotplotgenre.csv\n",
    "data = pd.read_csv(\"Datasets/onehotplotgenre.csv\")\n",
    "# Get the first 5 rows of the data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the plot column tokenized and lowercased\n",
    "tokenizeddf = data.copy()\n",
    "tokenizeddf['Plot'] = tokenizeddf['Plot'].apply(lambda x: nltk.word_tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Title  \\\n",
      "0                             # (2012/II)   \n",
      "1          #1 Cheerleader Camp (2010) (V)   \n",
      "2                 #1 Serial Killer (2013)   \n",
      "3  #1 at the Apocalypse Box Office (2015)   \n",
      "4                             #137 (2011)   \n",
      "\n",
      "                                                Plot  Action  Adventure  \\\n",
      "0  [the, night, falls, on, the, big, city, and, a...       0          0   \n",
      "1  [when, they, 're, hired, to, work, at, a, chee...       0          0   \n",
      "2  [years, of, seething, rage, against, the, raci...       0          0   \n",
      "3  [jules, is, ,, self, declared, ,, the, most, u...       0          0   \n",
      "4  [#, 137, is, a, sci/fi, thriller, about, a, gi...       0          0   \n",
      "\n",
      "   Animation  Biography  Comedy  Crime  Drama  Family  ...  Horror  Music  \\\n",
      "0          1          0       0      0      0       0  ...       0      0   \n",
      "1          0          0       1      0      0       0  ...       0      0   \n",
      "2          0          0       0      0      0       0  ...       1      0   \n",
      "3          0          0       1      0      0       0  ...       0      0   \n",
      "4          0          0       0      0      0       0  ...       0      0   \n",
      "\n",
      "   Musical  Mystery  Romance  Sci-Fi  Sport  Thriller  War  Western  \n",
      "0        0        0        0       0      0         0    0        0  \n",
      "1        0        0        0       0      0         0    0        0  \n",
      "2        0        0        0       0      0         0    0        0  \n",
      "3        0        0        0       1      0         0    0        0  \n",
      "4        0        0        0       1      0         0    0        0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizeddf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  520798\n"
     ]
    }
   ],
   "source": [
    "# Get the length of the longest plot\n",
    "maxlen = tokenizeddf['Plot'].apply(len).max()\n",
    "\n",
    "# Get the set of all words in the plot column\n",
    "wordset = set()\n",
    "for plot in tokenizeddf['Plot']:\n",
    "    wordset.update(plot)\n",
    "# Get the number of unique words\n",
    "numwords = len(wordset)\n",
    "print(\"Number of unique words: \", numwords)\n",
    "\n",
    "# Create a dictionary that maps words to integers\n",
    "word2int = {word: i for i, word in enumerate(wordset)}\n",
    "\n",
    "# Function to convert a list of words to a list of integers\n",
    "def words2ints(words):\n",
    "    return [word2int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Title  \\\n",
      "0                             # (2012/II)   \n",
      "1          #1 Cheerleader Camp (2010) (V)   \n",
      "2                 #1 Serial Killer (2013)   \n",
      "3  #1 at the Apocalypse Box Office (2015)   \n",
      "4                             #137 (2011)   \n",
      "\n",
      "                                                Plot  Action  Adventure  \\\n",
      "0  [73578, 197008, 406905, 259121, 73578, 77853, ...       0          0   \n",
      "1  [1887, 116539, 61002, 415459, 234436, 120942, ...       0          0   \n",
      "2  [284671, 443029, 510290, 278408, 29922, 73578,...       0          0   \n",
      "3  [29684, 386494, 159351, 111924, 8116, 159351, ...       0          0   \n",
      "4  [89540, 245013, 386494, 351170, 271014, 24025,...       0          0   \n",
      "\n",
      "   Animation  Biography  Comedy  Crime  Drama  Family  ...  Horror  Music  \\\n",
      "0          1          0       0      0      0       0  ...       0      0   \n",
      "1          0          0       1      0      0       0  ...       0      0   \n",
      "2          0          0       0      0      0       0  ...       1      0   \n",
      "3          0          0       1      0      0       0  ...       0      0   \n",
      "4          0          0       0      0      0       0  ...       0      0   \n",
      "\n",
      "   Musical  Mystery  Romance  Sci-Fi  Sport  Thriller  War  Western  \n",
      "0        0        0        0       0      0         0    0        0  \n",
      "1        0        0        0       0      0         0    0        0  \n",
      "2        0        0        0       0      0         0    0        0  \n",
      "3        0        0        0       1      0         0    0        0  \n",
      "4        0        0        0       1      0         0    0        0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the plot column to a list of integers\n",
    "tokenizeddf['Plot'] = tokenizeddf['Plot'].apply(words2ints)\n",
    "\n",
    "print(tokenizeddf.head())\n",
    "\n",
    "# Pad the plot column with zeros to make all plots the same length\n",
    "def pad_plot(plot):\n",
    "    return plot + [0] * (maxlen - len(plot))\n",
    "\n",
    "tokenizeddf['Plot'] = tokenizeddf['Plot'].apply(pad_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Title  \\\n",
      "0                             # (2012/II)   \n",
      "1          #1 Cheerleader Camp (2010) (V)   \n",
      "2                 #1 Serial Killer (2013)   \n",
      "3  #1 at the Apocalypse Box Office (2015)   \n",
      "4                             #137 (2011)   \n",
      "\n",
      "                                                Plot  Action  \n",
      "0  [73578, 197008, 406905, 259121, 73578, 77853, ...       0  \n",
      "1  [1887, 116539, 61002, 415459, 234436, 120942, ...       0  \n",
      "2  [284671, 443029, 510290, 278408, 29922, 73578,...       0  \n",
      "3  [29684, 386494, 159351, 111924, 8116, 159351, ...       0  \n",
      "4  [89540, 245013, 386494, 351170, 271014, 24025,...       0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\2112378440.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  actiondf[\"Action\"] = le.fit_transform(actiondf[\"Action\"])\n"
     ]
    }
   ],
   "source": [
    "# Make a dataframe with just title, plot, and action columns\n",
    "# Going to try this out first on just action movies\n",
    "actiondf = tokenizeddf[['Title', 'Plot', 'Action']]\n",
    "print(actiondf.head())\n",
    "\n",
    "# Encode the action column with LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "actiondf[\"Action\"] = le.fit_transform(actiondf[\"Action\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset class for the actiondata\n",
    "class GenreDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, genre):\n",
    "        self.plot = data[\"Plot\"].values\n",
    "        self.genre = data[genre].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.plot)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        plot = self.plot[i]\n",
    "        genre = self.genre[i]\n",
    "        return torch.tensor(plot, dtype=torch.long), torch.tensor(genre, dtype=torch.float)\n",
    "    \n",
    "# Train test split the actiondf\n",
    "from sklearn.model_selection import train_test_split\n",
    "actiontrain, actiontest = train_test_split(actiondf, test_size=0.2, random_state=42)\n",
    "    \n",
    "# Create a GenreDataset object for the actiondata\n",
    "actiontraindataset = GenreDataset(actiontrain, \"Action\")\n",
    "actiontestdataset = GenreDataset(actiontest, \"Action\")\n",
    "\n",
    "# Create a DataLoader for the actiondataset\n",
    "actiontrainloader = torch.utils.data.DataLoader(actiontraindataset, batch_size=32, shuffle=True)\n",
    "actiontestloader = torch.utils.data.DataLoader(actiontestdataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RNN to classify the plots as action or not\n",
    "class GenreRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
    "        super(GenreRNN, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(1, x.size(0), hidden).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])        \n",
    "        return out\n",
    "\n",
    "# Set the hyperparameters\n",
    "embed = 128\n",
    "hidden = 128\n",
    "output = 2\n",
    "\n",
    "# Create the model\n",
    "actionmodel = GenreRNN(numwords, embed, hidden, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.20157073438167572\n",
      "Epoch 2/5, Loss: 0.4095592498779297\n",
      "Epoch 3/5, Loss: 0.20661020278930664\n",
      "Epoch 4/5, Loss: 0.587815523147583\n",
      "Epoch 5/5, Loss: 0.19147908687591553\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(actionmodel.parameters(), lr=0.001)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Send the model to the device\n",
    "actionmodel.to(device)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    actionmodel.train()\n",
    "    for plots, genres in actiontrainloader:\n",
    "        # Send the data to the device\n",
    "        plots = plots.to(device)\n",
    "        genres = genres.to(device)\n",
    "        outputs = actionmodel(plots)\n",
    "        loss = criterion(outputs, genres.long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 32\n",
      "59 64\n",
      "88 96\n",
      "116 128\n",
      "145 160\n",
      "172 192\n",
      "203 224\n",
      "233 256\n",
      "261 288\n",
      "288 320\n",
      "317 352\n",
      "348 384\n",
      "376 416\n",
      "408 448\n",
      "436 480\n",
      "464 512\n",
      "491 544\n",
      "519 576\n",
      "548 608\n",
      "580 640\n",
      "609 672\n",
      "640 704\n",
      "668 736\n",
      "700 768\n",
      "729 800\n",
      "760 832\n",
      "790 864\n",
      "818 896\n",
      "845 928\n",
      "875 960\n",
      "907 992\n",
      "938 1024\n",
      "964 1056\n",
      "994 1088\n",
      "1024 1120\n",
      "1051 1152\n",
      "1079 1184\n",
      "1110 1216\n",
      "1140 1248\n",
      "1170 1280\n",
      "1199 1312\n",
      "1229 1344\n",
      "1256 1376\n",
      "1285 1408\n",
      "1315 1440\n",
      "1344 1472\n",
      "1374 1504\n",
      "1404 1536\n",
      "1434 1568\n",
      "1463 1600\n",
      "1492 1632\n",
      "1522 1664\n",
      "1550 1696\n",
      "1579 1728\n",
      "1611 1760\n",
      "1639 1792\n",
      "1666 1824\n",
      "1694 1856\n",
      "1724 1888\n",
      "1754 1920\n",
      "1786 1952\n",
      "1815 1984\n",
      "1843 2016\n",
      "1873 2048\n",
      "1904 2080\n",
      "1934 2112\n",
      "1965 2144\n",
      "1995 2176\n",
      "2019 2208\n",
      "2048 2240\n",
      "2077 2272\n",
      "2107 2304\n",
      "2139 2336\n",
      "2168 2368\n",
      "2199 2400\n",
      "2228 2432\n",
      "2257 2464\n",
      "2289 2496\n",
      "2320 2528\n",
      "2347 2560\n",
      "2377 2592\n",
      "2406 2624\n",
      "2434 2656\n",
      "2464 2688\n",
      "2490 2720\n",
      "2516 2752\n",
      "2544 2784\n",
      "2570 2816\n",
      "2601 2848\n",
      "2633 2880\n",
      "2660 2912\n",
      "2687 2944\n",
      "2717 2976\n",
      "2744 3008\n",
      "2772 3040\n",
      "2801 3072\n",
      "2832 3104\n",
      "2862 3136\n",
      "2892 3168\n",
      "2920 3200\n",
      "2949 3232\n",
      "2977 3264\n",
      "3006 3296\n",
      "3037 3328\n",
      "3062 3360\n",
      "3092 3392\n",
      "3120 3424\n",
      "3149 3456\n",
      "3179 3488\n",
      "3208 3520\n",
      "3238 3552\n",
      "3265 3584\n",
      "3296 3616\n",
      "3326 3648\n",
      "3354 3680\n",
      "3384 3712\n",
      "3412 3744\n",
      "3441 3776\n",
      "3470 3808\n",
      "3499 3840\n",
      "3525 3872\n",
      "3554 3904\n",
      "3584 3936\n",
      "3612 3968\n",
      "3643 4000\n",
      "3673 4032\n",
      "3703 4064\n",
      "3730 4096\n",
      "3760 4128\n",
      "3787 4160\n",
      "3816 4192\n",
      "3840 4224\n",
      "3870 4256\n",
      "3900 4288\n",
      "3929 4320\n",
      "3958 4352\n",
      "3986 4384\n",
      "4018 4416\n",
      "4049 4448\n",
      "4078 4480\n",
      "4109 4512\n",
      "4137 4544\n",
      "4168 4576\n",
      "4200 4608\n",
      "4230 4640\n",
      "4260 4672\n",
      "4289 4704\n",
      "4318 4736\n",
      "4345 4768\n",
      "4373 4800\n",
      "4403 4832\n",
      "4433 4864\n",
      "4461 4896\n",
      "4490 4928\n",
      "4519 4960\n",
      "4547 4992\n",
      "4574 5024\n",
      "4604 5056\n",
      "4633 5088\n",
      "4665 5120\n",
      "4694 5152\n",
      "4724 5184\n",
      "4756 5216\n",
      "4782 5248\n",
      "4812 5280\n",
      "4841 5312\n",
      "4872 5344\n",
      "4902 5376\n",
      "4931 5408\n",
      "4961 5440\n",
      "4990 5472\n",
      "5020 5504\n",
      "5049 5536\n",
      "5079 5568\n",
      "5107 5600\n",
      "5135 5632\n",
      "5165 5664\n",
      "5195 5696\n",
      "5223 5728\n",
      "5252 5760\n",
      "5279 5792\n",
      "5310 5824\n",
      "5339 5856\n",
      "5364 5888\n",
      "5393 5920\n",
      "5423 5952\n",
      "5454 5984\n",
      "5483 6016\n",
      "5514 6048\n",
      "5544 6080\n",
      "5573 6112\n",
      "5602 6144\n",
      "5634 6176\n",
      "5662 6208\n",
      "5689 6240\n",
      "5717 6272\n",
      "5748 6304\n",
      "5775 6336\n",
      "5803 6368\n",
      "5834 6400\n",
      "5861 6432\n",
      "5889 6464\n",
      "5921 6496\n",
      "5951 6528\n",
      "5980 6560\n",
      "6009 6592\n",
      "6037 6624\n",
      "6066 6656\n",
      "6096 6688\n",
      "6127 6720\n",
      "6157 6752\n",
      "6189 6784\n",
      "6218 6816\n",
      "6246 6848\n",
      "6278 6880\n",
      "6309 6912\n",
      "6341 6944\n",
      "6371 6976\n",
      "6397 7008\n",
      "6426 7040\n",
      "6454 7072\n",
      "6484 7104\n",
      "6514 7136\n",
      "6544 7168\n",
      "6576 7200\n",
      "6608 7232\n",
      "6639 7264\n",
      "6669 7296\n",
      "6699 7328\n",
      "6730 7360\n",
      "6759 7392\n",
      "6788 7424\n",
      "6818 7456\n",
      "6847 7488\n",
      "6877 7520\n",
      "6907 7552\n",
      "6937 7584\n",
      "6966 7616\n",
      "6996 7648\n",
      "7024 7680\n",
      "7054 7712\n",
      "7083 7744\n",
      "7114 7776\n",
      "7145 7808\n",
      "7175 7840\n",
      "7204 7872\n",
      "7234 7904\n",
      "7262 7936\n",
      "7293 7968\n",
      "7325 8000\n",
      "7355 8032\n",
      "7384 8064\n",
      "7412 8096\n",
      "7440 8128\n",
      "7467 8160\n",
      "7497 8192\n",
      "7524 8224\n",
      "7555 8256\n",
      "7584 8288\n",
      "7613 8320\n",
      "7643 8352\n",
      "7673 8384\n",
      "7704 8416\n",
      "7734 8448\n",
      "7763 8480\n",
      "7794 8512\n",
      "7823 8544\n",
      "7854 8576\n",
      "7881 8608\n",
      "7910 8640\n",
      "7938 8672\n",
      "7966 8704\n",
      "7995 8736\n",
      "8025 8768\n",
      "8056 8800\n",
      "8085 8832\n",
      "8113 8864\n",
      "8144 8896\n",
      "8174 8928\n",
      "8205 8960\n",
      "8233 8992\n",
      "8260 9024\n",
      "8290 9056\n",
      "8318 9088\n",
      "8349 9120\n",
      "8376 9152\n",
      "8407 9184\n",
      "8435 9216\n",
      "8461 9248\n",
      "8491 9280\n",
      "8519 9312\n",
      "8549 9344\n",
      "8579 9376\n",
      "8608 9408\n",
      "8638 9440\n",
      "8669 9472\n",
      "8698 9504\n",
      "8722 9536\n",
      "8750 9568\n",
      "8777 9600\n",
      "8805 9632\n",
      "8834 9664\n",
      "8864 9696\n",
      "8893 9728\n",
      "8922 9760\n",
      "8948 9792\n",
      "8977 9824\n",
      "9006 9856\n",
      "9036 9888\n",
      "9067 9920\n",
      "9096 9952\n",
      "9124 9984\n",
      "9152 10016\n",
      "9182 10048\n",
      "9210 10080\n",
      "9238 10112\n",
      "9267 10144\n",
      "9296 10176\n",
      "9324 10208\n",
      "9355 10240\n",
      "9382 10272\n",
      "9412 10304\n",
      "9440 10336\n",
      "9467 10368\n",
      "9499 10400\n",
      "9529 10432\n",
      "9556 10464\n",
      "9585 10496\n",
      "9615 10528\n",
      "9646 10560\n",
      "9676 10592\n",
      "9707 10624\n",
      "9738 10656\n",
      "9769 10688\n",
      "9798 10720\n",
      "9827 10752\n",
      "9856 10784\n",
      "9884 10816\n",
      "9915 10848\n",
      "9942 10880\n",
      "9969 10912\n",
      "10000 10944\n",
      "10029 10976\n",
      "10059 11008\n",
      "10089 11040\n",
      "10118 11072\n",
      "10150 11104\n",
      "10181 11136\n",
      "10209 11168\n",
      "10239 11200\n",
      "10266 11232\n",
      "10297 11264\n",
      "10327 11296\n",
      "10357 11328\n",
      "10387 11360\n",
      "10416 11392\n",
      "10446 11424\n",
      "10474 11456\n",
      "10506 11488\n",
      "10533 11520\n",
      "10563 11552\n",
      "10591 11584\n",
      "10620 11616\n",
      "10647 11648\n",
      "10676 11680\n",
      "10703 11712\n",
      "10732 11744\n",
      "10762 11776\n",
      "10791 11808\n",
      "10819 11840\n",
      "10850 11872\n",
      "10880 11904\n",
      "10909 11936\n",
      "10939 11968\n",
      "10969 12000\n",
      "10999 12032\n",
      "11028 12064\n",
      "11057 12096\n",
      "11087 12128\n",
      "11117 12160\n",
      "11147 12192\n",
      "11177 12224\n",
      "11207 12256\n",
      "11233 12288\n",
      "11264 12320\n",
      "11296 12352\n",
      "11325 12384\n",
      "11355 12416\n",
      "11385 12448\n",
      "11415 12480\n",
      "11444 12512\n",
      "11472 12544\n",
      "11499 12576\n",
      "11528 12608\n",
      "11556 12640\n",
      "11585 12672\n",
      "11614 12704\n",
      "11641 12736\n",
      "11669 12768\n",
      "11700 12800\n",
      "11728 12832\n",
      "11756 12864\n",
      "11785 12896\n",
      "11813 12928\n",
      "11844 12960\n",
      "11871 12992\n",
      "11900 13024\n",
      "11931 13056\n",
      "11961 13088\n",
      "11990 13120\n",
      "12022 13152\n",
      "12052 13184\n",
      "12083 13216\n",
      "12112 13248\n",
      "12140 13280\n",
      "12168 13312\n",
      "12199 13344\n",
      "12225 13376\n",
      "12254 13408\n",
      "12284 13440\n",
      "12314 13472\n",
      "12345 13504\n",
      "12375 13536\n",
      "12402 13568\n",
      "12434 13600\n",
      "12464 13632\n",
      "12495 13664\n",
      "12524 13696\n",
      "12552 13728\n",
      "12580 13760\n",
      "12609 13792\n",
      "12638 13824\n",
      "12668 13856\n",
      "12698 13888\n",
      "12726 13920\n",
      "12756 13952\n",
      "12787 13984\n",
      "12819 14016\n",
      "12847 14048\n",
      "12875 14080\n",
      "12905 14112\n",
      "12934 14144\n",
      "12964 14176\n",
      "12995 14208\n",
      "13023 14240\n",
      "13051 14272\n",
      "13082 14304\n",
      "13109 14336\n",
      "13137 14368\n",
      "13169 14400\n",
      "13198 14432\n",
      "13229 14464\n",
      "13260 14496\n",
      "13289 14528\n",
      "13317 14560\n",
      "13347 14592\n",
      "13378 14624\n",
      "13410 14656\n",
      "13439 14688\n",
      "13468 14720\n",
      "13498 14752\n",
      "13528 14784\n",
      "13554 14816\n",
      "13585 14848\n",
      "13616 14880\n",
      "13646 14912\n",
      "13674 14944\n",
      "13704 14976\n",
      "13729 15008\n",
      "13759 15040\n",
      "13787 15072\n",
      "13817 15104\n",
      "13847 15136\n",
      "13875 15168\n",
      "13906 15200\n",
      "13935 15232\n",
      "13965 15264\n",
      "13997 15296\n",
      "14026 15328\n",
      "14056 15360\n",
      "14083 15392\n",
      "14110 15424\n",
      "14140 15456\n",
      "14172 15488\n",
      "14204 15520\n",
      "14233 15552\n",
      "14263 15584\n",
      "14290 15616\n",
      "14322 15648\n",
      "14349 15680\n",
      "14375 15712\n",
      "14406 15744\n",
      "14435 15776\n",
      "14466 15808\n",
      "14497 15840\n",
      "14524 15872\n",
      "14553 15904\n",
      "14582 15936\n",
      "14608 15968\n",
      "14637 16000\n",
      "14665 16032\n",
      "14692 16064\n",
      "14722 16096\n",
      "14753 16128\n",
      "14784 16160\n",
      "14813 16192\n",
      "14842 16224\n",
      "14871 16256\n",
      "14902 16288\n",
      "14931 16320\n",
      "14961 16352\n",
      "14991 16384\n",
      "15022 16416\n",
      "15051 16448\n",
      "15083 16480\n",
      "15115 16512\n",
      "15140 16544\n",
      "15169 16576\n",
      "15199 16608\n",
      "15230 16640\n",
      "15260 16672\n",
      "15290 16704\n",
      "15320 16736\n",
      "15346 16768\n",
      "15375 16800\n",
      "15405 16832\n",
      "15435 16864\n",
      "15466 16896\n",
      "15494 16928\n",
      "15522 16960\n",
      "15549 16992\n",
      "15581 17024\n",
      "15609 17056\n",
      "15637 17088\n",
      "15668 17120\n",
      "15697 17152\n",
      "15728 17184\n",
      "15758 17216\n",
      "15789 17248\n",
      "15817 17280\n",
      "15845 17312\n",
      "15871 17344\n",
      "15899 17376\n",
      "15929 17408\n",
      "15956 17440\n",
      "15986 17472\n",
      "16013 17504\n",
      "16042 17536\n",
      "16073 17568\n",
      "16103 17600\n",
      "16135 17632\n",
      "16165 17664\n",
      "16194 17696\n",
      "16225 17728\n",
      "16254 17760\n",
      "16282 17792\n",
      "16310 17824\n",
      "16339 17856\n",
      "16370 17888\n",
      "16396 17920\n",
      "16426 17952\n",
      "16454 17984\n",
      "16483 18016\n",
      "16511 18048\n",
      "16540 18080\n",
      "16566 18112\n",
      "16598 18144\n",
      "16630 18176\n",
      "16661 18208\n",
      "16692 18240\n",
      "16722 18272\n",
      "16747 18304\n",
      "16776 18336\n",
      "16805 18368\n",
      "16835 18400\n",
      "16867 18432\n",
      "16898 18464\n",
      "16928 18496\n",
      "16958 18528\n",
      "16987 18560\n",
      "17016 18592\n",
      "17045 18624\n",
      "17074 18656\n",
      "17103 18688\n",
      "17134 18720\n",
      "17162 18752\n",
      "17193 18784\n",
      "17224 18816\n",
      "17252 18848\n",
      "17283 18880\n",
      "17311 18912\n",
      "17343 18944\n",
      "17373 18976\n",
      "17402 19008\n",
      "17430 19040\n",
      "17460 19072\n",
      "17489 19104\n",
      "17518 19136\n",
      "17550 19168\n",
      "17580 19200\n",
      "17609 19232\n",
      "17638 19264\n",
      "17668 19296\n",
      "17696 19328\n",
      "17725 19360\n",
      "17752 19392\n",
      "17784 19424\n",
      "17813 19456\n",
      "17842 19488\n",
      "17872 19520\n",
      "17902 19552\n",
      "17928 19584\n",
      "17958 19616\n",
      "17988 19648\n",
      "18018 19680\n",
      "18049 19712\n",
      "18077 19744\n",
      "18106 19776\n",
      "18137 19808\n",
      "18164 19840\n",
      "18193 19872\n",
      "18224 19904\n",
      "18253 19936\n",
      "18282 19968\n",
      "18309 20000\n",
      "18339 20032\n",
      "18370 20064\n",
      "18399 20096\n",
      "18430 20128\n",
      "18458 20160\n",
      "18487 20192\n",
      "18513 20224\n",
      "18541 20256\n",
      "18566 20288\n",
      "18596 20320\n",
      "18626 20352\n",
      "18657 20384\n",
      "18684 20416\n",
      "18713 20448\n",
      "18744 20480\n",
      "18774 20512\n",
      "18801 20544\n",
      "18832 20576\n",
      "18860 20608\n",
      "18889 20640\n",
      "18917 20672\n",
      "18948 20704\n",
      "18978 20736\n",
      "19008 20768\n",
      "19036 20800\n",
      "19062 20832\n",
      "19090 20864\n",
      "19119 20896\n",
      "19148 20928\n",
      "19178 20960\n",
      "19206 20992\n",
      "19237 21024\n",
      "19266 21056\n",
      "19297 21088\n",
      "19327 21120\n",
      "19358 21152\n",
      "19388 21184\n",
      "19415 21216\n",
      "19446 21248\n",
      "19476 21280\n",
      "19507 21312\n",
      "19535 21344\n",
      "19564 21376\n",
      "19595 21408\n",
      "19626 21440\n",
      "19653 21472\n",
      "19681 21504\n",
      "19710 21536\n",
      "19741 21568\n",
      "19770 21600\n",
      "19799 21632\n",
      "19829 21664\n",
      "19858 21696\n",
      "19887 21728\n",
      "19917 21760\n",
      "19946 21792\n",
      "19976 21824\n",
      "20004 21856\n",
      "20033 21888\n",
      "20061 21920\n",
      "20090 21952\n",
      "20120 21984\n",
      "20151 22016\n",
      "20182 22048\n",
      "20214 22080\n",
      "20246 22112\n",
      "20276 22144\n",
      "20305 22176\n",
      "20333 22208\n",
      "20363 22240\n",
      "20393 22272\n",
      "20424 22304\n",
      "20453 22336\n",
      "20484 22368\n",
      "20511 22400\n",
      "20541 22432\n",
      "20571 22464\n",
      "20600 22496\n",
      "20630 22528\n",
      "20660 22560\n",
      "20690 22592\n",
      "20718 22624\n",
      "20747 22656\n",
      "20775 22688\n",
      "20803 22720\n",
      "20832 22752\n",
      "20862 22784\n",
      "20888 22816\n",
      "20919 22848\n",
      "20949 22880\n",
      "20978 22912\n",
      "21006 22944\n",
      "21036 22976\n",
      "21065 23008\n",
      "21093 23040\n",
      "21123 23072\n",
      "21153 23104\n",
      "21182 23136\n",
      "21213 23168\n",
      "21240 23200\n",
      "21272 23232\n",
      "21303 23264\n",
      "21330 23296\n",
      "21362 23328\n",
      "21393 23360\n",
      "21425 23392\n",
      "21456 23424\n",
      "21487 23456\n",
      "21515 23488\n",
      "21547 23520\n",
      "21576 23552\n",
      "21606 23584\n",
      "21636 23616\n",
      "21667 23648\n",
      "21699 23680\n",
      "21728 23712\n",
      "21757 23744\n",
      "21785 23776\n",
      "21811 23808\n",
      "21839 23840\n",
      "21870 23872\n",
      "21900 23904\n",
      "21926 23936\n",
      "21954 23968\n",
      "21983 24000\n",
      "22011 24032\n",
      "22041 24064\n",
      "22067 24096\n",
      "22097 24128\n",
      "22126 24160\n",
      "22158 24192\n",
      "22184 24224\n",
      "22213 24256\n",
      "22242 24288\n",
      "22271 24320\n",
      "22299 24352\n",
      "22329 24384\n",
      "22358 24416\n",
      "22387 24448\n",
      "22418 24480\n",
      "22448 24512\n",
      "22478 24544\n",
      "22505 24576\n",
      "22533 24608\n",
      "22565 24640\n",
      "22594 24672\n",
      "22625 24704\n",
      "22654 24736\n",
      "22682 24768\n",
      "22712 24800\n",
      "22743 24832\n",
      "22771 24864\n",
      "22800 24896\n",
      "22828 24928\n",
      "22857 24960\n",
      "22888 24992\n",
      "22919 25024\n",
      "22947 25056\n",
      "22975 25088\n",
      "23003 25120\n",
      "23035 25152\n",
      "23067 25184\n",
      "23095 25216\n",
      "23124 25248\n",
      "23155 25280\n",
      "23185 25312\n",
      "23217 25344\n",
      "23245 25376\n",
      "23276 25408\n",
      "23306 25440\n",
      "23334 25472\n",
      "23362 25504\n",
      "23389 25536\n",
      "23419 25568\n",
      "23448 25600\n",
      "23476 25632\n",
      "23507 25664\n",
      "23536 25696\n",
      "23567 25728\n",
      "23597 25760\n",
      "23626 25792\n",
      "23651 25824\n",
      "23681 25856\n",
      "23709 25888\n",
      "23738 25920\n",
      "23768 25952\n",
      "23797 25984\n",
      "23828 26016\n",
      "23856 26048\n",
      "23886 26080\n",
      "23916 26112\n",
      "23944 26144\n",
      "23973 26176\n",
      "24003 26208\n",
      "24033 26240\n",
      "24063 26272\n",
      "24091 26304\n",
      "24122 26336\n",
      "24151 26368\n",
      "24181 26400\n",
      "24210 26432\n",
      "24240 26464\n",
      "24267 26496\n",
      "24296 26528\n",
      "24325 26560\n",
      "24352 26592\n",
      "24379 26624\n",
      "24407 26656\n",
      "24434 26688\n",
      "24463 26720\n",
      "24491 26752\n",
      "24523 26784\n",
      "24551 26816\n",
      "24583 26848\n",
      "24613 26880\n",
      "24643 26912\n",
      "24671 26944\n",
      "24700 26976\n",
      "24730 27008\n",
      "24757 27040\n",
      "24788 27072\n",
      "24818 27104\n",
      "24848 27136\n",
      "24876 27168\n",
      "24906 27200\n",
      "24936 27232\n",
      "24965 27264\n",
      "24995 27296\n",
      "25026 27328\n",
      "25055 27360\n",
      "25084 27392\n",
      "25114 27424\n",
      "25141 27456\n",
      "25170 27488\n",
      "25198 27520\n",
      "25229 27552\n",
      "25259 27584\n",
      "25288 27616\n",
      "25320 27648\n",
      "25350 27680\n",
      "25381 27712\n",
      "25410 27744\n",
      "25439 27776\n",
      "25470 27808\n",
      "25502 27840\n",
      "25534 27872\n",
      "25565 27904\n",
      "25596 27936\n",
      "25627 27968\n",
      "25656 28000\n",
      "25687 28032\n",
      "25716 28064\n",
      "25746 28096\n",
      "25775 28128\n",
      "25804 28160\n",
      "25835 28192\n",
      "25866 28224\n",
      "25895 28256\n",
      "25926 28288\n",
      "25956 28320\n",
      "25988 28352\n",
      "26018 28384\n",
      "26049 28416\n",
      "26079 28448\n",
      "26110 28480\n",
      "26141 28512\n",
      "26171 28544\n",
      "26201 28576\n",
      "26230 28608\n",
      "26260 28640\n",
      "26289 28672\n",
      "26319 28704\n",
      "26347 28736\n",
      "26377 28768\n",
      "26407 28800\n",
      "26437 28832\n",
      "26467 28864\n",
      "26498 28896\n",
      "26528 28928\n",
      "26555 28960\n",
      "26585 28992\n",
      "26615 29024\n",
      "26645 29056\n",
      "26671 29088\n",
      "26702 29120\n",
      "26729 29152\n",
      "26756 29184\n",
      "26787 29216\n",
      "26818 29248\n",
      "26849 29280\n",
      "26878 29312\n",
      "26909 29344\n",
      "26937 29376\n",
      "26966 29408\n",
      "26996 29440\n",
      "27024 29472\n",
      "27053 29504\n",
      "27083 29536\n",
      "27113 29568\n",
      "27141 29600\n",
      "27172 29632\n",
      "27202 29664\n",
      "27233 29696\n",
      "27262 29728\n",
      "27294 29760\n",
      "27322 29792\n",
      "27351 29824\n",
      "27383 29856\n",
      "27413 29888\n",
      "27442 29920\n",
      "27471 29952\n",
      "27501 29984\n",
      "27532 30016\n",
      "27560 30048\n",
      "27590 30080\n",
      "27617 30112\n",
      "27648 30144\n",
      "27680 30176\n",
      "27708 30208\n",
      "27738 30240\n",
      "27767 30272\n",
      "27795 30304\n",
      "27823 30336\n",
      "27852 30368\n",
      "27878 30400\n",
      "27909 30432\n",
      "27938 30464\n",
      "27969 30496\n",
      "27999 30528\n",
      "28030 30560\n",
      "28060 30592\n",
      "28086 30624\n",
      "28116 30656\n",
      "28146 30688\n",
      "28176 30720\n",
      "28206 30752\n",
      "28237 30784\n",
      "28264 30816\n",
      "28293 30848\n",
      "28322 30880\n",
      "28354 30912\n",
      "28384 30944\n",
      "28411 30976\n",
      "28439 31008\n",
      "28471 31040\n",
      "28503 31072\n",
      "28533 31104\n",
      "28562 31136\n",
      "28592 31168\n",
      "28621 31200\n",
      "28650 31232\n",
      "28678 31264\n",
      "28704 31296\n",
      "28735 31328\n",
      "28765 31360\n",
      "28795 31392\n",
      "28825 31424\n",
      "28855 31456\n",
      "28885 31488\n",
      "28915 31520\n",
      "28947 31552\n",
      "28977 31584\n",
      "29007 31616\n",
      "29036 31648\n",
      "29064 31680\n",
      "29095 31712\n",
      "29126 31744\n",
      "29155 31776\n",
      "29184 31808\n",
      "29213 31840\n",
      "29244 31872\n",
      "29275 31904\n",
      "29303 31936\n",
      "29334 31968\n",
      "29366 32000\n",
      "29397 32032\n",
      "29426 32064\n",
      "29455 32096\n",
      "29485 32128\n",
      "29516 32160\n",
      "29545 32192\n",
      "29572 32224\n",
      "29599 32256\n",
      "29627 32288\n",
      "29653 32320\n",
      "29681 32352\n",
      "29710 32384\n",
      "29739 32416\n",
      "29769 32448\n",
      "29795 32480\n",
      "29825 32512\n",
      "29853 32544\n",
      "29882 32576\n",
      "29912 32608\n",
      "29942 32640\n",
      "29971 32672\n",
      "30000 32704\n",
      "30026 32736\n",
      "30057 32768\n",
      "30085 32800\n",
      "30114 32832\n",
      "30145 32864\n",
      "30174 32896\n",
      "30203 32928\n",
      "30230 32960\n",
      "30261 32992\n",
      "30292 33024\n",
      "30320 33056\n",
      "30351 33088\n",
      "30381 33120\n",
      "30410 33152\n",
      "30440 33184\n",
      "30471 33216\n",
      "30499 33248\n",
      "30527 33280\n",
      "30556 33312\n",
      "30587 33344\n",
      "30617 33376\n",
      "30648 33408\n",
      "30679 33440\n",
      "30709 33472\n",
      "30741 33504\n",
      "30770 33536\n",
      "30797 33568\n",
      "30827 33600\n",
      "30855 33632\n",
      "30884 33664\n",
      "30911 33696\n",
      "30942 33728\n",
      "30973 33760\n",
      "31004 33792\n",
      "31032 33824\n",
      "31064 33856\n",
      "31093 33888\n",
      "31124 33920\n",
      "31155 33952\n",
      "31184 33984\n",
      "31207 34016\n",
      "31236 34048\n",
      "31265 34080\n",
      "31295 34112\n",
      "31325 34144\n",
      "31356 34176\n",
      "31383 34208\n",
      "31413 34240\n",
      "31441 34272\n",
      "31471 34304\n",
      "31501 34336\n",
      "31530 34368\n",
      "31560 34400\n",
      "31591 34432\n",
      "31623 34464\n",
      "31652 34496\n",
      "31682 34528\n",
      "31713 34560\n",
      "31745 34592\n",
      "31776 34624\n",
      "31806 34656\n",
      "31834 34688\n",
      "31862 34720\n",
      "31891 34752\n",
      "31919 34784\n",
      "31946 34816\n",
      "31973 34848\n",
      "32003 34880\n",
      "32035 34912\n",
      "32065 34944\n",
      "32094 34976\n",
      "32123 35008\n",
      "32153 35040\n",
      "32182 35072\n",
      "32213 35104\n",
      "32242 35136\n",
      "32271 35168\n",
      "32301 35200\n",
      "32332 35232\n",
      "32361 35264\n",
      "32392 35296\n",
      "32423 35328\n",
      "32452 35360\n",
      "32481 35392\n",
      "32511 35424\n",
      "32542 35456\n",
      "32572 35488\n",
      "32602 35520\n",
      "32630 35552\n",
      "32661 35584\n",
      "32690 35616\n",
      "32721 35648\n",
      "32747 35680\n",
      "32779 35712\n",
      "32809 35744\n",
      "32837 35776\n",
      "32867 35808\n",
      "32896 35840\n",
      "32925 35872\n",
      "32954 35904\n",
      "32985 35936\n",
      "33015 35968\n",
      "33044 36000\n",
      "33072 36032\n",
      "33101 36064\n",
      "33130 36096\n",
      "33160 36128\n",
      "33190 36160\n",
      "33222 36192\n",
      "33252 36224\n",
      "33279 36256\n",
      "33308 36288\n",
      "33335 36320\n",
      "33366 36352\n",
      "33395 36384\n",
      "33424 36416\n",
      "33454 36448\n",
      "33484 36480\n",
      "33511 36512\n",
      "33540 36544\n",
      "33566 36576\n",
      "33595 36608\n",
      "33623 36640\n",
      "33654 36672\n",
      "33682 36704\n",
      "33712 36736\n",
      "33742 36768\n",
      "33773 36800\n",
      "33804 36832\n",
      "33833 36864\n",
      "33860 36896\n",
      "33890 36928\n",
      "33920 36960\n",
      "33950 36992\n",
      "33980 37024\n",
      "34010 37056\n",
      "34041 37088\n",
      "34068 37120\n",
      "34098 37152\n",
      "34128 37184\n",
      "34157 37216\n",
      "34187 37248\n",
      "34216 37280\n",
      "34242 37312\n",
      "34273 37344\n",
      "34301 37376\n",
      "34330 37408\n",
      "34362 37440\n",
      "34393 37472\n",
      "34421 37504\n",
      "34446 37536\n",
      "34476 37568\n",
      "34506 37600\n",
      "34534 37632\n",
      "34562 37664\n",
      "34591 37696\n",
      "34619 37728\n",
      "34648 37760\n",
      "34674 37792\n",
      "34705 37824\n",
      "34735 37856\n",
      "34763 37888\n",
      "34792 37920\n",
      "34823 37952\n",
      "34851 37984\n",
      "34881 38016\n",
      "34910 38048\n",
      "34940 38080\n",
      "34971 38112\n",
      "34997 38144\n",
      "35026 38176\n",
      "35054 38208\n",
      "35085 38240\n",
      "35115 38272\n",
      "35141 38304\n",
      "35173 38336\n",
      "35201 38368\n",
      "35229 38400\n",
      "35260 38432\n",
      "35291 38464\n",
      "35322 38496\n",
      "35352 38528\n",
      "35384 38560\n",
      "35412 38592\n",
      "35442 38624\n",
      "35469 38656\n",
      "35496 38688\n",
      "35525 38720\n",
      "35556 38752\n",
      "35588 38784\n",
      "35619 38816\n",
      "35647 38848\n",
      "35675 38880\n",
      "35704 38912\n",
      "35733 38944\n",
      "35763 38976\n",
      "35795 39008\n",
      "35825 39040\n",
      "35857 39072\n",
      "35883 39104\n",
      "35910 39136\n",
      "35938 39168\n",
      "35968 39200\n",
      "35998 39232\n",
      "36029 39264\n",
      "36058 39296\n",
      "36090 39328\n",
      "36119 39360\n",
      "36149 39392\n",
      "36179 39424\n",
      "36209 39456\n",
      "36236 39488\n",
      "36267 39520\n",
      "36295 39552\n",
      "36327 39584\n",
      "36358 39616\n",
      "36387 39648\n",
      "36418 39680\n",
      "36450 39712\n",
      "36478 39744\n",
      "36507 39776\n",
      "36538 39808\n",
      "36567 39840\n",
      "36598 39872\n",
      "36628 39904\n",
      "36657 39936\n",
      "36686 39968\n",
      "36714 40000\n",
      "36744 40032\n",
      "36771 40064\n",
      "36802 40096\n",
      "36832 40128\n",
      "36861 40160\n",
      "36893 40192\n",
      "36923 40224\n",
      "36953 40256\n",
      "36982 40288\n",
      "37010 40320\n",
      "37040 40352\n",
      "37070 40384\n",
      "37098 40416\n",
      "37128 40448\n",
      "37158 40480\n",
      "37186 40512\n",
      "37215 40544\n",
      "37245 40576\n",
      "37275 40608\n",
      "37304 40640\n",
      "37335 40672\n",
      "37364 40704\n",
      "37395 40736\n",
      "37426 40768\n",
      "37458 40800\n",
      "37484 40832\n",
      "37514 40864\n",
      "37545 40896\n",
      "37574 40928\n",
      "37603 40960\n",
      "37633 40992\n",
      "37661 41024\n",
      "37690 41056\n",
      "37719 41088\n",
      "37748 41120\n",
      "37778 41152\n",
      "37810 41184\n",
      "37841 41216\n",
      "37869 41248\n",
      "37898 41280\n",
      "37926 41312\n",
      "37956 41344\n",
      "37984 41376\n",
      "38016 41408\n",
      "38046 41440\n",
      "38074 41472\n",
      "38104 41504\n",
      "38135 41536\n",
      "38165 41568\n",
      "38194 41600\n",
      "38224 41632\n",
      "38255 41664\n",
      "38284 41696\n",
      "38312 41728\n",
      "38342 41760\n",
      "38372 41792\n",
      "38402 41824\n",
      "38433 41856\n",
      "38462 41888\n",
      "38491 41920\n",
      "38520 41952\n",
      "38548 41984\n",
      "38579 42016\n",
      "38605 42048\n",
      "38634 42080\n",
      "38663 42112\n",
      "38695 42144\n",
      "38724 42176\n",
      "38752 42208\n",
      "38782 42240\n",
      "38811 42272\n",
      "38842 42304\n",
      "38873 42336\n",
      "38903 42368\n",
      "38934 42400\n",
      "38963 42432\n",
      "38992 42464\n",
      "39020 42496\n",
      "39050 42528\n",
      "39078 42560\n",
      "39107 42592\n",
      "39135 42624\n",
      "39163 42656\n",
      "39193 42688\n",
      "39222 42720\n",
      "39252 42752\n",
      "39282 42784\n",
      "39312 42816\n",
      "39339 42848\n",
      "39368 42880\n",
      "39399 42912\n",
      "39427 42944\n",
      "39456 42976\n",
      "39486 43008\n",
      "39517 43040\n",
      "39549 43072\n",
      "39578 43104\n",
      "39606 43136\n",
      "39633 43168\n",
      "39662 43200\n",
      "39692 43232\n",
      "39724 43264\n",
      "39754 43296\n",
      "39782 43328\n",
      "39813 43360\n",
      "39844 43392\n",
      "39872 43424\n",
      "39903 43456\n",
      "39935 43488\n",
      "39965 43520\n",
      "39994 43552\n",
      "40024 43584\n",
      "40052 43616\n",
      "40082 43648\n",
      "40113 43680\n",
      "40143 43712\n",
      "40170 43744\n",
      "40199 43776\n",
      "40228 43808\n",
      "40258 43840\n",
      "40290 43872\n",
      "40317 43904\n",
      "40347 43936\n",
      "40376 43968\n",
      "40405 44000\n",
      "40433 44032\n",
      "40463 44064\n",
      "40493 44096\n",
      "40522 44128\n",
      "40549 44160\n",
      "40578 44192\n",
      "40609 44224\n",
      "40638 44256\n",
      "40665 44288\n",
      "40697 44320\n",
      "40726 44352\n",
      "40758 44384\n",
      "40787 44416\n",
      "40813 44448\n",
      "40843 44480\n",
      "40874 44512\n",
      "40903 44544\n",
      "40932 44576\n",
      "40962 44608\n",
      "40993 44640\n",
      "41023 44672\n",
      "41051 44704\n",
      "41079 44736\n",
      "41108 44768\n",
      "41135 44800\n",
      "41161 44832\n",
      "41190 44864\n",
      "41221 44896\n",
      "41253 44928\n",
      "41285 44960\n",
      "41316 44992\n",
      "41344 45024\n",
      "41369 45056\n",
      "41398 45088\n",
      "41427 45120\n",
      "41454 45152\n",
      "41483 45184\n",
      "41510 45216\n",
      "41539 45248\n",
      "41569 45280\n",
      "41599 45312\n",
      "41628 45344\n",
      "41656 45376\n",
      "41686 45408\n",
      "41715 45440\n",
      "41744 45472\n",
      "41774 45504\n",
      "41806 45536\n",
      "41835 45568\n",
      "41864 45600\n",
      "41893 45632\n",
      "41924 45664\n",
      "41950 45696\n",
      "41980 45728\n",
      "42007 45760\n",
      "42032 45792\n",
      "42062 45824\n",
      "42091 45856\n",
      "42119 45888\n",
      "42149 45920\n",
      "42178 45952\n",
      "42206 45984\n",
      "42235 46016\n",
      "42263 46048\n",
      "42291 46080\n",
      "42321 46112\n",
      "42352 46144\n",
      "42383 46176\n",
      "42413 46208\n",
      "42444 46240\n",
      "42474 46272\n",
      "42503 46304\n",
      "42532 46336\n",
      "42561 46368\n",
      "42592 46400\n",
      "42621 46432\n",
      "42650 46464\n",
      "42680 46496\n",
      "42710 46528\n",
      "42740 46560\n",
      "42769 46592\n",
      "42798 46624\n",
      "42828 46656\n",
      "42856 46688\n",
      "42887 46720\n",
      "42917 46752\n",
      "42947 46784\n",
      "42976 46816\n",
      "43005 46848\n",
      "43034 46880\n",
      "43065 46912\n",
      "43096 46944\n",
      "43126 46976\n",
      "43154 47008\n",
      "43181 47040\n",
      "43210 47072\n",
      "43241 47104\n",
      "43269 47136\n",
      "43299 47168\n",
      "43326 47200\n",
      "43356 47232\n",
      "43386 47264\n",
      "43415 47296\n",
      "43446 47328\n",
      "43475 47360\n",
      "43504 47392\n",
      "43536 47424\n",
      "43566 47456\n",
      "43597 47488\n",
      "43626 47520\n",
      "43657 47552\n",
      "43687 47584\n",
      "43717 47616\n",
      "43746 47648\n",
      "43778 47680\n",
      "43809 47712\n",
      "43839 47744\n",
      "43868 47776\n",
      "43898 47808\n",
      "43929 47840\n",
      "43957 47872\n",
      "43987 47904\n",
      "44013 47936\n",
      "44041 47968\n",
      "44071 48000\n",
      "44101 48032\n",
      "44131 48064\n",
      "44159 48096\n",
      "44189 48128\n",
      "44216 48160\n",
      "44248 48192\n",
      "44276 48224\n",
      "44305 48256\n",
      "44334 48288\n",
      "44363 48320\n",
      "44393 48352\n",
      "44422 48384\n",
      "44451 48416\n",
      "44478 48448\n",
      "44506 48480\n",
      "44537 48512\n",
      "44568 48544\n",
      "44598 48576\n",
      "44629 48608\n",
      "44658 48640\n",
      "44685 48672\n",
      "44714 48704\n",
      "44744 48736\n",
      "44772 48768\n",
      "44803 48800\n",
      "44834 48832\n",
      "44862 48864\n",
      "44893 48896\n",
      "44923 48928\n",
      "44950 48960\n",
      "44979 48992\n",
      "45009 49024\n",
      "45039 49056\n",
      "45065 49088\n",
      "45095 49120\n",
      "45126 49152\n",
      "45156 49184\n",
      "45181 49216\n",
      "45208 49248\n",
      "45239 49280\n",
      "45271 49312\n",
      "45300 49344\n",
      "45330 49376\n",
      "45359 49408\n",
      "45387 49440\n",
      "45416 49472\n",
      "45445 49504\n",
      "45477 49536\n",
      "45507 49568\n",
      "45533 49600\n",
      "45560 49632\n",
      "45589 49664\n",
      "45620 49696\n",
      "45650 49728\n",
      "45679 49760\n",
      "45709 49792\n",
      "45738 49824\n",
      "45767 49856\n",
      "45798 49888\n",
      "45825 49920\n",
      "45855 49952\n",
      "45885 49984\n",
      "45914 50016\n",
      "45945 50048\n",
      "45971 50080\n",
      "45999 50112\n",
      "46028 50144\n",
      "46055 50176\n",
      "46081 50208\n",
      "46108 50240\n",
      "46138 50272\n",
      "46166 50304\n",
      "46193 50336\n",
      "46223 50368\n",
      "46249 50400\n",
      "46280 50432\n",
      "46308 50464\n",
      "46339 50496\n",
      "46369 50528\n",
      "46399 50560\n",
      "46428 50592\n",
      "46458 50624\n",
      "46488 50656\n",
      "46517 50688\n",
      "46546 50720\n",
      "46575 50752\n",
      "46601 50784\n",
      "46632 50816\n",
      "46664 50848\n",
      "46692 50880\n",
      "46723 50912\n",
      "46751 50944\n",
      "46782 50976\n",
      "46808 51008\n",
      "46838 51040\n",
      "46870 51072\n",
      "46898 51104\n",
      "46926 51136\n",
      "46956 51168\n",
      "46984 51200\n",
      "47015 51232\n",
      "47044 51264\n",
      "47073 51296\n",
      "47099 51328\n",
      "47126 51360\n",
      "47153 51392\n",
      "47183 51424\n",
      "47213 51456\n",
      "47245 51488\n",
      "47273 51520\n",
      "47305 51552\n",
      "47335 51584\n",
      "47359 51616\n",
      "47389 51648\n",
      "47417 51680\n",
      "47445 51712\n",
      "47473 51744\n",
      "47501 51776\n",
      "47530 51808\n",
      "47559 51840\n",
      "47588 51872\n",
      "47616 51904\n",
      "47645 51936\n",
      "47675 51968\n",
      "47705 52000\n",
      "47735 52032\n",
      "47766 52064\n",
      "47794 52096\n",
      "47821 52128\n",
      "47851 52160\n",
      "47880 52192\n",
      "47910 52224\n",
      "47939 52256\n",
      "47969 52288\n",
      "48000 52320\n",
      "48030 52352\n",
      "48059 52384\n",
      "48086 52416\n",
      "48115 52448\n",
      "48141 52480\n",
      "48170 52512\n",
      "48197 52544\n",
      "48225 52576\n",
      "48255 52608\n",
      "48284 52640\n",
      "48312 52672\n",
      "48344 52704\n",
      "48374 52736\n",
      "48402 52768\n",
      "48432 52800\n",
      "48463 52832\n",
      "48491 52864\n",
      "48520 52896\n",
      "48548 52928\n",
      "48578 52960\n",
      "48609 52992\n",
      "48638 53024\n",
      "48668 53056\n",
      "48697 53088\n",
      "48729 53120\n",
      "48758 53152\n",
      "48787 53184\n",
      "48815 53216\n",
      "48844 53248\n",
      "48874 53280\n",
      "48906 53312\n",
      "48938 53344\n",
      "48968 53376\n",
      "48999 53408\n",
      "49028 53440\n",
      "49055 53472\n",
      "49081 53504\n",
      "49110 53536\n",
      "49139 53568\n",
      "49168 53600\n",
      "49194 53632\n",
      "49220 53664\n",
      "49251 53696\n",
      "49278 53728\n",
      "49308 53760\n",
      "49337 53792\n",
      "49367 53824\n",
      "49395 53856\n",
      "49423 53888\n",
      "49451 53920\n",
      "49479 53952\n",
      "49508 53984\n",
      "49538 54016\n",
      "49566 54048\n",
      "49597 54080\n",
      "49627 54112\n",
      "49657 54144\n",
      "49687 54176\n",
      "49712 54208\n",
      "49742 54240\n",
      "49772 54272\n",
      "49803 54304\n",
      "49831 54336\n",
      "49861 54368\n",
      "49891 54400\n",
      "49920 54432\n",
      "49951 54464\n",
      "49980 54496\n",
      "50011 54528\n",
      "50042 54560\n",
      "50071 54592\n",
      "50101 54624\n",
      "50133 54656\n",
      "50162 54688\n",
      "50191 54720\n",
      "50222 54752\n",
      "50253 54784\n",
      "50284 54816\n",
      "50314 54848\n",
      "50344 54880\n",
      "50374 54912\n",
      "50404 54944\n",
      "50433 54976\n",
      "50462 55008\n",
      "50493 55040\n",
      "50521 55072\n",
      "50550 55104\n",
      "50579 55136\n",
      "50611 55168\n",
      "50640 55200\n",
      "50669 55232\n",
      "50699 55264\n",
      "50728 55296\n",
      "50757 55328\n",
      "50788 55360\n",
      "50818 55392\n",
      "50849 55424\n",
      "50879 55456\n",
      "50910 55488\n",
      "50941 55520\n",
      "50969 55552\n",
      "50999 55584\n",
      "51031 55616\n",
      "51061 55648\n",
      "51090 55680\n",
      "51120 55712\n",
      "51150 55744\n",
      "51181 55776\n",
      "51212 55808\n",
      "51242 55840\n",
      "51272 55872\n",
      "51301 55904\n",
      "51329 55936\n",
      "51359 55968\n",
      "51391 56000\n",
      "51419 56032\n",
      "51450 56064\n",
      "51481 56096\n",
      "51511 56128\n",
      "51534 56160\n",
      "51562 56192\n",
      "51592 56224\n",
      "51622 56256\n",
      "51651 56288\n",
      "51682 56320\n",
      "51712 56352\n",
      "51738 56384\n",
      "51766 56416\n",
      "51796 56448\n",
      "51825 56480\n",
      "51853 56512\n",
      "51884 56544\n",
      "51909 56576\n",
      "51936 56608\n",
      "51966 56640\n",
      "51995 56672\n",
      "52022 56704\n",
      "52053 56736\n",
      "52081 56768\n",
      "52110 56800\n",
      "52141 56832\n",
      "52170 56864\n",
      "52199 56896\n",
      "52228 56928\n",
      "52260 56960\n",
      "52288 56992\n",
      "52317 57024\n",
      "52347 57056\n",
      "52377 57088\n",
      "52404 57120\n",
      "52434 57152\n",
      "52464 57184\n",
      "52493 57216\n",
      "52522 57248\n",
      "52552 57280\n",
      "52583 57312\n",
      "52613 57344\n",
      "52644 57376\n",
      "52672 57408\n",
      "52703 57440\n",
      "52731 57472\n",
      "52760 57504\n",
      "52791 57536\n",
      "52823 57568\n",
      "52850 57600\n",
      "52879 57632\n",
      "52909 57664\n",
      "52941 57696\n",
      "52971 57728\n",
      "53001 57760\n",
      "53032 57792\n",
      "53061 57824\n",
      "53089 57856\n",
      "53121 57888\n",
      "53152 57920\n",
      "53180 57952\n",
      "53206 57984\n",
      "53235 58016\n",
      "53265 58048\n",
      "53293 58080\n",
      "53321 58112\n",
      "53351 58144\n",
      "53381 58176\n",
      "53411 58208\n",
      "53441 58240\n",
      "53469 58272\n",
      "53501 58304\n",
      "53531 58336\n",
      "53560 58368\n",
      "53585 58400\n",
      "53615 58432\n",
      "53644 58464\n",
      "53675 58496\n",
      "53705 58528\n",
      "53733 58560\n",
      "53763 58592\n",
      "53795 58624\n",
      "53823 58656\n",
      "53854 58688\n",
      "53885 58720\n",
      "53914 58752\n",
      "53945 58784\n",
      "53972 58816\n",
      "54003 58848\n",
      "54032 58880\n",
      "54061 58912\n",
      "54090 58944\n",
      "54119 58976\n",
      "54149 59008\n",
      "54179 59040\n",
      "54209 59072\n",
      "54239 59104\n",
      "54267 59136\n",
      "54298 59168\n",
      "54326 59200\n",
      "54355 59232\n",
      "54383 59264\n",
      "54409 59296\n",
      "54437 59328\n",
      "54467 59360\n",
      "54498 59392\n",
      "54528 59424\n",
      "54557 59456\n",
      "54589 59488\n",
      "54617 59520\n",
      "54649 59552\n",
      "54677 59584\n",
      "54704 59616\n",
      "54733 59648\n",
      "54757 59680\n",
      "54788 59712\n",
      "54820 59744\n",
      "54850 59776\n",
      "54880 59808\n",
      "54911 59840\n",
      "54939 59872\n",
      "54967 59904\n",
      "54994 59936\n",
      "55023 59968\n",
      "55053 60000\n",
      "55081 60032\n",
      "55107 60064\n",
      "55138 60096\n",
      "55168 60128\n",
      "55197 60160\n",
      "55225 60192\n",
      "55253 60224\n",
      "55280 60256\n",
      "55308 60288\n",
      "55336 60320\n",
      "55366 60352\n",
      "55393 60384\n",
      "55423 60416\n",
      "55455 60448\n",
      "55485 60480\n",
      "55514 60512\n",
      "55542 60544\n",
      "55573 60576\n",
      "55603 60608\n",
      "55633 60640\n",
      "55661 60672\n",
      "55690 60704\n",
      "55720 60736\n",
      "55749 60768\n",
      "55780 60800\n",
      "55811 60832\n",
      "55842 60864\n",
      "55870 60896\n",
      "55902 60928\n",
      "55933 60960\n",
      "55962 60992\n",
      "55993 61024\n",
      "56024 61056\n",
      "56051 61088\n",
      "56080 61120\n",
      "56110 61152\n",
      "56138 61184\n",
      "56166 61216\n",
      "56197 61248\n",
      "56226 61280\n",
      "56258 61312\n",
      "56287 61344\n",
      "56316 61376\n",
      "56344 61408\n",
      "56375 61440\n",
      "56407 61472\n",
      "56434 61504\n",
      "56464 61536\n",
      "56493 61568\n",
      "56520 61600\n",
      "56550 61632\n",
      "56580 61664\n",
      "56610 61696\n",
      "56638 61728\n",
      "56669 61760\n",
      "56697 61792\n",
      "56726 61824\n",
      "56754 61856\n",
      "56782 61888\n",
      "56809 61920\n",
      "56841 61952\n",
      "56870 61984\n",
      "56900 62016\n",
      "56927 62048\n",
      "56958 62080\n",
      "56984 62112\n",
      "57014 62144\n",
      "57044 62176\n",
      "57074 62208\n",
      "57102 62240\n",
      "57132 62272\n",
      "57162 62304\n",
      "57193 62336\n",
      "57222 62368\n",
      "57253 62400\n",
      "57283 62432\n",
      "57312 62464\n",
      "57339 62496\n",
      "57367 62528\n",
      "57396 62560\n",
      "57422 62592\n",
      "57452 62624\n",
      "57481 62656\n",
      "57510 62688\n",
      "57541 62720\n",
      "57572 62752\n",
      "57600 62784\n",
      "57627 62816\n",
      "57652 62848\n",
      "57681 62880\n",
      "57711 62912\n",
      "57738 62944\n",
      "57767 62976\n",
      "57797 63008\n",
      "57827 63040\n",
      "57858 63072\n",
      "57888 63104\n",
      "57917 63136\n",
      "57947 63168\n",
      "57975 63200\n",
      "58006 63232\n",
      "58032 63264\n",
      "58064 63296\n",
      "58092 63328\n",
      "58120 63360\n",
      "58149 63392\n",
      "58175 63424\n",
      "58202 63456\n",
      "58231 63488\n",
      "58259 63520\n",
      "58288 63552\n",
      "58316 63584\n",
      "58347 63616\n",
      "58378 63648\n",
      "58407 63680\n",
      "58436 63712\n",
      "58467 63744\n",
      "58494 63776\n",
      "58520 63808\n",
      "58548 63840\n",
      "58579 63872\n",
      "58608 63904\n",
      "58636 63936\n",
      "58667 63968\n",
      "58699 64000\n",
      "58729 64032\n",
      "58760 64064\n",
      "58788 64096\n",
      "58817 64128\n",
      "58845 64160\n",
      "58871 64192\n",
      "58903 64224\n",
      "58931 64256\n",
      "58962 64288\n",
      "58991 64320\n",
      "59021 64352\n",
      "59050 64384\n",
      "59080 64416\n",
      "59112 64448\n",
      "59143 64480\n",
      "59174 64512\n",
      "59203 64544\n",
      "59232 64576\n",
      "59261 64608\n",
      "59289 64640\n",
      "59318 64672\n",
      "59347 64704\n",
      "59378 64736\n",
      "59407 64768\n",
      "59437 64800\n",
      "59466 64832\n",
      "59497 64864\n",
      "59526 64896\n",
      "59552 64928\n",
      "59580 64960\n",
      "59611 64992\n",
      "59642 65024\n",
      "59672 65056\n",
      "59701 65088\n",
      "59731 65120\n",
      "59760 65152\n",
      "59792 65184\n",
      "59821 65216\n",
      "59851 65248\n",
      "59879 65280\n",
      "59909 65312\n",
      "59937 65344\n",
      "59967 65376\n",
      "59995 65408\n",
      "60024 65440\n",
      "60052 65472\n",
      "60081 65504\n",
      "60110 65536\n",
      "60138 65568\n",
      "60166 65600\n",
      "60195 65632\n",
      "60224 65664\n",
      "60252 65696\n",
      "60282 65728\n",
      "60312 65760\n",
      "60341 65792\n",
      "60372 65824\n",
      "60401 65856\n",
      "60431 65888\n",
      "60460 65920\n",
      "60490 65952\n",
      "60520 65984\n",
      "60550 66016\n",
      "60581 66048\n",
      "60613 66080\n",
      "60644 66112\n",
      "60674 66144\n",
      "60704 66176\n",
      "60734 66208\n",
      "60766 66240\n",
      "60795 66272\n",
      "60824 66304\n",
      "60849 66336\n",
      "60876 66368\n",
      "60904 66400\n",
      "60927 66432\n",
      "60956 66464\n",
      "60986 66496\n",
      "61016 66528\n",
      "61043 66560\n",
      "61074 66592\n",
      "61104 66624\n",
      "61133 66656\n",
      "61161 66688\n",
      "61173 66702\n",
      "Accuracy: 0.9171089322658991\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "actionmodel.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "# Send the model to the device\n",
    "actionmodel.to(device)\n",
    "# Don't calculate gradients\n",
    "with torch.no_grad():\n",
    "    for plots, genres in actiontestloader:\n",
    "        # Send the data to the device\n",
    "        plots = plots.to(device)\n",
    "        genres = genres.to(device)\n",
    "        outputs = actionmodel(plots)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += genres.size(0)\n",
    "        num_correct = (predicted == genres).sum().item()\n",
    "        correct += num_correct\n",
    "        print(correct, total)\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# Create the models directory if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(\"models/rnn\"):\n",
    "    os.makedirs(\"models/rnn\")\n",
    "torch.save(actionmodel.state_dict(), \"models/rnn/actionmodel.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we'll train an RNN for each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a model\n",
    "def train_genre_rnn(genre, epochs, embed, hidden, output):\n",
    "    genremodel = GenreRNN(numwords, embed, hidden, output)\n",
    "    # Extract the relevant columns from the dataframe\n",
    "    genredf = tokenizeddf[['Title', 'Plot', genre]]\n",
    "    # Encode the genre column\n",
    "    genredf[genre] = le.fit_transform(genredf[genre])\n",
    "    # Train test split the data\n",
    "    # We won't use the test data in this function\n",
    "    genretrain, genretest = train_test_split(genredf, test_size=0.2, random_state=42)\n",
    "    # Create a GenreDataset object\n",
    "    genretraindataset = GenreDataset(genretrain, genre)\n",
    "    # Create a DataLoader\n",
    "    genretrainloader = torch.utils.data.DataLoader(genretraindataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(actionmodel.parameters(), lr=0.001)\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Send the model to the device\n",
    "    genremodel.to(device)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        genremodel.train()\n",
    "        for plots, genres in genretrainloader:\n",
    "            # Send the data to the device\n",
    "            plots = plots.to(device)\n",
    "            genres = genres.to(device)\n",
    "            outputs = genremodel(plots)\n",
    "            loss = criterion(outputs, genres.long())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Return the model\n",
    "    return genremodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model\n",
    "def save_model(model, genre):\n",
    "    if not os.path.exists(\"models/rnn\"):\n",
    "        os.makedirs(\"models/rnn\")\n",
    "    torch.save(model.state_dict(), f\"models/rnn/{genre}model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime',\n",
      "       'Drama', 'Family', 'Fantasy', 'History', 'Horror', 'Music', 'Musical',\n",
      "       'Mystery', 'Romance', 'Sci-Fi', 'Sport', 'Thriller', 'War', 'Western'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Get all the genres\n",
    "genres = data.columns[2:]\n",
    "print(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for Action\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.5859911441802979\n",
      "Epoch 2/5, Loss: 0.5859912633895874\n",
      "Epoch 3/5, Loss: 0.6159600019454956\n",
      "Epoch 4/5, Loss: 0.5660120248794556\n",
      "Epoch 5/5, Loss: 0.5959808826446533\n",
      "Training model for Adventure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7564619183540344\n",
      "Epoch 2/5, Loss: 0.7471478581428528\n",
      "Epoch 3/5, Loss: 0.7564618587493896\n",
      "Epoch 4/5, Loss: 0.7657760381698608\n",
      "Epoch 5/5, Loss: 0.7704330086708069\n",
      "Training model for Animation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.2833893299102783\n",
      "Epoch 2/5, Loss: 1.3194283246994019\n",
      "Epoch 3/5, Loss: 1.4275447130203247\n",
      "Epoch 4/5, Loss: 1.4275447130203247\n",
      "Epoch 5/5, Loss: 1.3554670810699463\n",
      "Training model for Biography\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.5439463257789612\n",
      "Epoch 2/5, Loss: 0.5439463257789612\n",
      "Epoch 3/5, Loss: 0.5439463257789612\n",
      "Epoch 4/5, Loss: 0.5658823251724243\n",
      "Epoch 5/5, Loss: 0.5549143552780151\n",
      "Training model for Comedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.779388427734375\n",
      "Epoch 2/5, Loss: 0.8075230717658997\n",
      "Epoch 3/5, Loss: 0.7606320381164551\n",
      "Epoch 4/5, Loss: 0.770010232925415\n",
      "Epoch 5/5, Loss: 0.7981448769569397\n",
      "Training model for Crime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6334042549133301\n",
      "Epoch 2/5, Loss: 0.6375324130058289\n",
      "Epoch 3/5, Loss: 0.6416606307029724\n",
      "Epoch 4/5, Loss: 0.6292760968208313\n",
      "Epoch 5/5, Loss: 0.6375324726104736\n",
      "Training model for Drama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6874387264251709\n",
      "Epoch 2/5, Loss: 0.7233567833900452\n",
      "Epoch 3/5, Loss: 0.6994114518165588\n",
      "Epoch 4/5, Loss: 0.6395480632781982\n",
      "Epoch 5/5, Loss: 0.7353294491767883\n",
      "Training model for Family\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.083617091178894\n",
      "Epoch 2/5, Loss: 1.160685420036316\n",
      "Epoch 3/5, Loss: 1.160685420036316\n",
      "Epoch 4/5, Loss: 1.083617091178894\n",
      "Epoch 5/5, Loss: 1.1093065738677979\n",
      "Training model for Fantasy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.47848665714263916\n",
      "Epoch 2/5, Loss: 0.4991147816181183\n",
      "Epoch 3/5, Loss: 0.49911484122276306\n",
      "Epoch 4/5, Loss: 0.47848665714263916\n",
      "Epoch 5/5, Loss: 0.4991148114204407\n",
      "Training model for History\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3367787003517151\n",
      "Epoch 2/5, Loss: 0.3044980764389038\n",
      "Epoch 3/5, Loss: 0.3367787003517151\n",
      "Epoch 4/5, Loss: 0.3044980764389038\n",
      "Epoch 5/5, Loss: 0.36905938386917114\n",
      "Training model for Horror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.0949739217758179\n",
      "Epoch 2/5, Loss: 1.051993489265442\n",
      "Epoch 3/5, Loss: 1.0305033922195435\n",
      "Epoch 4/5, Loss: 1.051993727684021\n",
      "Epoch 5/5, Loss: 1.030503511428833\n",
      "Training model for Music\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7184355854988098\n",
      "Epoch 2/5, Loss: 0.720212996006012\n",
      "Epoch 3/5, Loss: 0.720212996006012\n",
      "Epoch 4/5, Loss: 0.7184355854988098\n",
      "Epoch 5/5, Loss: 0.7184355854988098\n",
      "Training model for Musical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6820958256721497\n",
      "Epoch 2/5, Loss: 0.6827903985977173\n",
      "Epoch 3/5, Loss: 0.6827903985977173\n",
      "Epoch 4/5, Loss: 0.6834849715232849\n",
      "Epoch 5/5, Loss: 0.6820958256721497\n",
      "Training model for Mystery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.8887199759483337\n",
      "Epoch 2/5, Loss: 0.8887199759483337\n",
      "Epoch 3/5, Loss: 0.8662773370742798\n",
      "Epoch 4/5, Loss: 0.843834638595581\n",
      "Epoch 5/5, Loss: 0.8774986267089844\n",
      "Training model for Romance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.8458174467086792\n",
      "Epoch 2/5, Loss: 0.8458174467086792\n",
      "Epoch 3/5, Loss: 0.8358218669891357\n",
      "Epoch 4/5, Loss: 0.8458173274993896\n",
      "Epoch 5/5, Loss: 0.8158310651779175\n",
      "Training model for Sci-Fi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.8220991492271423\n",
      "Epoch 2/5, Loss: 0.8494167327880859\n",
      "Epoch 3/5, Loss: 0.8220991492271423\n",
      "Epoch 4/5, Loss: 0.8312050104141235\n",
      "Epoch 5/5, Loss: 0.8038874864578247\n",
      "Training model for Sport\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6407902836799622\n",
      "Epoch 2/5, Loss: 0.6407904028892517\n",
      "Epoch 3/5, Loss: 0.6407904028892517\n",
      "Epoch 4/5, Loss: 0.6371892690658569\n",
      "Epoch 5/5, Loss: 0.6407903432846069\n",
      "Training model for Thriller\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7093785405158997\n",
      "Epoch 2/5, Loss: 0.7056778073310852\n",
      "Epoch 3/5, Loss: 0.7093785405158997\n",
      "Epoch 4/5, Loss: 0.7106121182441711\n",
      "Epoch 5/5, Loss: 0.7093785405158997\n",
      "Training model for War\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7915686964988708\n",
      "Epoch 2/5, Loss: 0.8048267960548401\n",
      "Epoch 3/5, Loss: 0.7915686368942261\n",
      "Epoch 4/5, Loss: 0.7981977462768555\n",
      "Epoch 5/5, Loss: 0.7849395275115967\n",
      "Training model for Western\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zkand\\AppData\\Local\\Temp\\ipykernel_9328\\1652754105.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genredf[genre] = le.fit_transform(genredf[genre])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.626314103603363\n",
      "Epoch 2/5, Loss: 0.626314103603363\n",
      "Epoch 3/5, Loss: 0.6306408047676086\n",
      "Epoch 4/5, Loss: 0.626314103603363\n",
      "Epoch 5/5, Loss: 0.6263141632080078\n"
     ]
    }
   ],
   "source": [
    "# For each genre, check if there's already a model\n",
    "# If not, train a model and save it\n",
    "for genre in genres:\n",
    "    if not os.path.exists(f\"models/rnn/{genre}model.pth\"):\n",
    "        print(f\"Training model for {genre}\")\n",
    "        model = train_genre_rnn(genre, 5, 128, 128, 2)\n",
    "        save_model(model, genre)\n",
    "    else:\n",
    "        print(f\"Model for {genre} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old stuff below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[263], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m actiondata \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Reduce actiondata to only have the first 1000 rows\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#actiondata = actiondata.head(1000)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Tokenize the plot column\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m actiondata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlot\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mactiondata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Get the first 5 rows of the actiondata\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(actiondata\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\zkand\\miniconda3\\lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zkand\\miniconda3\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zkand\\miniconda3\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\zkand\\miniconda3\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zkand\\miniconda3\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zkand\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\zkand\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\zkand\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\destructive.py:160\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    157\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPUNCTUATION:\n\u001b[1;32m--> 160\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Handles parentheses.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m regexp, substitution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPARENS_BRACKETS\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Make a dataframe that has only the title, plot, and action columns\n",
    "actiondata = data[[\"Title\", \"Plot\", \"Action\"]]\n",
    "# Get the first 5 rows of the actiondata\n",
    "print(actiondata.head())\n",
    "\n",
    "# Encode the action column with LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "actiondata[\"Action\"] = le.fit_transform(actiondata[\"Action\"])\n",
    "\n",
    "# Get the length of the longest plot (for padding later on)\n",
    "maxlen = actiondata[\"Plot\"].apply(len).max()\n",
    "\n",
    "# Get the set of all words in the plot column\n",
    "wordset = set()\n",
    "for plot in actiondata[\"Plot\"]:\n",
    "    wordset.update(plot)\n",
    "# Get the number of unique words\n",
    "numwords = len(wordset)\n",
    "print(\"Number of unique words:\", numwords)\n",
    "\n",
    "# Create a dictionary that maps words to integers\n",
    "word2int = {word: i for i, word in enumerate(wordset)}\n",
    "\n",
    "# Function to convert a list of words to a list of integers\n",
    "def words2ints(words):\n",
    "    return [word2int[word] for word in words]\n",
    "\n",
    "# Convert the plot column to a list of integers\n",
    "actiondata[\"Plot\"] = actiondata[\"Plot\"].apply(words2ints)\n",
    "\n",
    "# Get the first 5 rows of the actiondata\n",
    "print(actiondata.head())\n",
    "\n",
    "# Pad the plot column with zeros to make all plots the same length\n",
    "def pad_plot(plot):\n",
    "    return plot + [0] * (maxlen - len(plot))\n",
    "\n",
    "actiondata[\"Plot\"] = actiondata[\"Plot\"].apply(pad_plot)\n",
    "\n",
    "# Get the first 5 rows of the actiondata\n",
    "print(actiondata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset class for the actiondata\n",
    "class GenreDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, genre):\n",
    "        self.plot = data[\"Plot\"].values\n",
    "        self.genre = data[genre].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.plot)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        plot = self.plot[i]\n",
    "        genre = self.genre[i]\n",
    "        return torch.tensor(plot, dtype=torch.long), torch.tensor(genre, dtype=torch.float)\n",
    "    \n",
    "# Create a GenreDataset object for the actiondata\n",
    "actiondataset = GenreDataset(actiondata, \"Action\")\n",
    "\n",
    "# Create a DataLoader for the actiondataset\n",
    "actionloader = torch.utils.data.DataLoader(actiondataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([10155, 15944, 14626,  ...,     0,     0,     0]), tensor(0.))\n"
     ]
    }
   ],
   "source": [
    "# Create the RNN to classify the plots as action or not\n",
    "class GenreRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
    "        super(GenreRNN, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(1, x.size(0), hidden).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])        \n",
    "        return out\n",
    "\n",
    "# Set the hyperparameters\n",
    "embed = 128\n",
    "hidden = 128\n",
    "output = 2\n",
    "\n",
    "# Create the model\n",
    "actionmodel = GenreRNN(numwords, embed, hidden, output)\n",
    "\n",
    "print(actiondataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.38106486201286316\n",
      "Epoch 2/10, Loss: 0.13124583661556244\n",
      "Epoch 3/10, Loss: 0.08345924317836761\n",
      "Epoch 4/10, Loss: 0.09120485931634903\n",
      "Epoch 5/10, Loss: 0.6385515332221985\n",
      "Epoch 6/10, Loss: 0.3833125829696655\n",
      "Epoch 7/10, Loss: 0.07289722561836243\n",
      "Epoch 8/10, Loss: 0.12186636030673981\n",
      "Epoch 9/10, Loss: 0.3818072974681854\n",
      "Epoch 10/10, Loss: 0.38340580463409424\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(actionmodel.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    actionmodel.train()\n",
    "    for plots, genres in actionloader:\n",
    "        outputs = actionmodel(plots)\n",
    "        loss = criterion(outputs, genres.long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(1)\n",
      "1\n",
      "29 correct out of 32\n"
     ]
    }
   ],
   "source": [
    "# Look at the first five plots and their predicted genres\n",
    "actionmodel.eval()\n",
    "with torch.no_grad():\n",
    "    for plots, genres in actionloader:\n",
    "        outputs = actionmodel(plots)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        #print(\"Predictions:\", preds)\n",
    "        #print(\"Actual:\", genres)\n",
    "        print((preds == genres)[0])\n",
    "        print((preds == genres)[0].sum())\n",
    "        print((preds == genres)[0].sum().item())\n",
    "        num_correct = (preds == genres).sum().item()\n",
    "        print(num_correct, \"correct out of\", len(genres))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "916 1000\n",
      "Accuracy: 0.916\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "actionmodel.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for plots, genres in actionloader:\n",
    "        outputs = actionmodel(plots)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += genres.size(0)\n",
    "        num_correct = (predicted == genres).sum().item()\n",
    "        correct += num_correct\n",
    "print(correct, total)\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
